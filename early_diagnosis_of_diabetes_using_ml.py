# -*- coding: utf-8 -*-
"""Early diagnosis of diabetes using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQLLf74UwreFqwnDVN5qf6ze6bVsLOlf
"""

from google.colab import files
uploaded = files.upload()

"""Import the necessary libraries"""

import warnings
warnings.filterwarnings('ignore')

# Import Neccessary libraries
import numpy as np
import pandas as pd

# Import Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

#Import Model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline

#Import Sampler libraries
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as imbPipeline

# Set the decimal format
pd.options.display.float_format = "{:.2f}".format

# Commented out IPython magic to ensure Python compatibility.


# %matplotlib inline

import seaborn as sns



#Import Model

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.preprocessing import  LabelEncoder

from sklearn.preprocessing import MinMaxScaler

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report

from sklearn.linear_model import LogisticRegression

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.svm import SVC

from sklearn.ensemble import GradientBoostingClassifier

df = pd.read_csv('diabetes_prediction_dataset.csv')

df.head()

"""## Lets do some EDA"""

# Handle duplicates in the data
duplicate_rows_data = df[df.duplicated()]
print("number of duplicate rows: ", duplicate_rows_data.shape)

df = df.drop_duplicates()

# Show distinct values
for column in df.columns:
    num_distinct_values = len(df[column].unique())
    print(f"{column}: {num_distinct_values} distinct values")

print(df.isnull().sum())

# Remove useless values
df = df[df['gender'] != 'Other']

df.describe().style.format("{:.2f}")

"""## Plot some graphs to have a better picture"""

# Histogram for age
plt.hist(df['age'], bins=30, edgecolor='black')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

# Bar plot for gender
sns.countplot(x='gender', data=df)
plt.title('Gender Distribution')
plt.show()

# Distribution plot for BMI
sns.distplot(df['bmi'], bins=30)
plt.title('BMI Distribution')
plt.show()

# Count plots for binary variables
for col in ['hypertension', 'heart_disease', 'diabetes']:
    sns.countplot(x=col, data=df)
    plt.title(f'{col} Distribution')
    plt.show()

# Count plot for smoking history
sns.countplot(x='smoking_history', data=df)
plt.title('Smoking History Distribution')
plt.show()

# Boxplot BMI vs Diabetes classification
sns.boxplot(x='diabetes', y='bmi', data=df)
plt.title('BMI vs Diabetes')
plt.show()

# Boxplot Age vs Diabetes
sns.boxplot(x='diabetes', y='age', data=df)
plt.title('Age vs Diabetes')
plt.show()

# Count plot of gender vs diabetes
sns.countplot(x='gender', hue='diabetes', data=df)
plt.title('Gender vs Diabetes')
plt.show()

# Boxplot HbA1c level vs Diabetes classification
sns.boxplot(x='diabetes', y='HbA1c_level', data=df)
plt.title('HbA1c level vs Diabetes')
plt.show()

# Boxplot blood glucose level vs Diabetes
sns.boxplot(x='diabetes', y='blood_glucose_level', data=df)
plt.title('Blood Glucose Level vs Diabetes')
plt.show()

"""###Multivariate Analysis ###"""

# Scatterplot Age vs BMI colored by Diabetes classification
sns.scatterplot(x='age', y='bmi', hue='diabetes', data=df)
plt.title('Age vs BMI')
plt.show()

# Violin plot of BMI vs diabetes classification by gender
sns.violinplot(x='diabetes', y='bmi', hue='gender', split=True, data=df)
plt.title('BMI vs Diabetes split by Gender')
plt.show()

# Interaction between gender, BMI and diabetes
sns.boxplot(x='diabetes', y='bmi', hue='gender', data=df)
plt.title('BMI Distribution by Diabetes Status and Gender')
plt.show()

# Interaction between gender, Age and diabetes
sns.boxplot(x='diabetes', y='age', hue='gender', data=df)
plt.title('Age Distribution by Diabetes Status and Gender')
plt.show()

"""##Correlation"""

# Function to map the existing categories to new ones
def recategorize_smoking(smoking_status):
    if smoking_status in ['never', 'No Info']:
        return 'non-smoker'
    elif smoking_status == 'current':
        return 'current'
    elif smoking_status in ['ever', 'former', 'not current']:
        return 'past_smoker'

# Apply to the 'smoking_history' column
df['smoking_history'] = df['smoking_history'].apply(recategorize_smoking)

# New value counts
print(df['smoking_history'].value_counts())

data = df.copy()

def perform_one_hot_encoding(df, column_name):

    dummies = pd.get_dummies(df[column_name], prefix=column_name)
    df = pd.concat([df.drop(column_name, axis=1), dummies], axis=1)

    return df

# one-hot encoding for gender
data = perform_one_hot_encoding(data, 'gender')

# for smoking history
data = perform_one_hot_encoding(data, 'smoking_history')

# correlation matrix
correlation_matrix = data.corr()

plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')
plt.title("Correlation Matrix Heatmap")
plt.show()


# create a heatmap
corr = data.corr()
target_corr = corr['diabetes'].drop('diabetes')

# Sorting correlation values in descending order
target_corr_sorted = target_corr.sort_values(ascending=False)

sns.set(font_scale=0.8)
sns.set_style("white")
sns.set_palette("PuBuGn_d")
sns.heatmap(target_corr_sorted.to_frame(), cmap="coolwarm", annot=True, fmt='.2f')
plt.title('Correlation with Diabetes')
plt.show()

"""##Handling class imbalance"""

# Count plot for the 'diabetes'
sns.countplot(x='diabetes', data=df)
plt.title('Diabetes Distribution')
plt.show()

over = SMOTE(sampling_strategy=0.1)
under = RandomUnderSampler(sampling_strategy=0.5)

"""###Preprocessing - might have to remove this one"""

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level','hypertension','heart_disease']),
        ('cat', OneHotEncoder(), ['gender','smoking_history'])
    ])

# Split data into features and target variable
X = df.drop('diabetes', axis=1)
y = df['diabetes']

#converting categorical data into numerical data

encoder=LabelEncoder()
df['gender']=encoder.fit_transform(df['gender'])
df['smoking_history']=encoder.fit_transform(df['smoking_history'])

"""##Splitting data"""

X = df.drop('diabetes', axis=1)
y = df['diabetes']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = 0.2)

X_train.shape,X_test.shape,X.shape

"""#Logistic Regression model"""

lr=LogisticRegression(max_iter=3000)
lr.fit(X_train,y_train)
y_predection=lr.predict(X_test)

lr_accuracy = accuracy_score(y_test, y_predection)
lr_conf_matrix = confusion_matrix(y_test, y_predection)
lr_classification_rep = classification_report(y_test, y_predection)

print(f'lr_Accuracy: {lr_accuracy:.2f}')
print('\nlr_Confusion Matrix:')
print(lr_conf_matrix)
print('\nlr_Classification Report:')
print(lr_classification_rep)

print("Training Score:",lr.score(X_train,y_train)*100,'%')
print("Testing Score:",lr.score(X_test,y_test)*100,'%')

"""## Random Forest"""

# parameter grid to search

param_grid_ = {
    'n_estimators': [10, 50, 300, 200],  # Number of trees
    'max_depth': [None, 10, 20, 30],    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]

}

random_forest_model = GridSearchCV(RandomForestClassifier(), param_grid=param_grid_, cv=2, n_jobs=-1)

random_forest_model = RandomForestClassifier()

random_forest_model.fit(X_train,y_train)

y_pred_rf=random_forest_model.predict(X_test)

rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_conf_matrix = confusion_matrix(y_test, y_pred_rf)
rf_classification_rep = classification_report(y_test, y_pred_rf)

print(f'rf_Accuracy: {rf_accuracy:.3f}')
print('\nrf_Confusion Matrix:')
print(rf_conf_matrix)
print('\nrf_Classification Report:')
print(rf_classification_rep)

print("Training Score:",random_forest_model.score(X_train,y_train)*100,'%')
print("Testing Score:",random_forest_model.score(X_test,y_test)*100,'%')

"""## SVM"""

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Use the same features and target
X = df.drop('diabetes', axis=1)
y = df['diabetes']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

svm_model = SVC(kernel='linear', probability=True, max_iter=3000, random_state=42)
svm_model.fit(X_train, y_train)

y_pred_svm = svm_model.predict(X_test)

svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)
svm_classification_rep = classification_report(y_test, y_pred_svm)

print("Linear SVM Accuracy:", svm_accuracy)
print("Confusion Matrix:\n", svm_conf_matrix)
print("Classification Report:\n", svm_classification_rep)

"""##Ensemble method Soft voting"""

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report



#Create soft voting ensemble
ensemble = VotingClassifier(
    estimators=[
        ('lr', lr),
        ('rf', random_forest_model),
        ('svm', svm_model)
    ],
    voting='soft'
)

# Train ensemble
ensemble.fit(X_train, y_train)

# Evaluate
y_pred = ensemble.predict(X_test)
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

"""## SHAP (SHapley Additive exPlanations)"""

!pip install shap --quiet

import shap

# Choose one model first â€” like Random Forest
model_to_explain = random_forest_model  # or lr_model or svm_model

# If needed, re-fit on full training data
model_to_explain.fit(X_train, y_train)

explainer = shap.TreeExplainer(model_to_explain)
shap_values = explainer.shap_values(X_test)

# For LR or SVM
model_to_explain = lr
explainer = shap.KernelExplainer(model_to_explain.predict_proba, X_train[:100])  # subset for speed
shap_values = explainer.shap_values(X_test[:50])  # also subset test set

#For summary
# it will use feature names from df
shap.summary_plot(shap_values, X_test[:50], feature_names=X.columns)

